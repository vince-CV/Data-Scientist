{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts content -> Feature Extraction -> ?\n",
    "\n",
    "### Textual information\n",
    "\n",
    "取决于使用的模型类型：<br>\n",
    "* Graph based model to extract insights -> symbolic nodes with relationship (**WordNet**);<br>\n",
    "* Statistical model -> numerical representation.<br>\n",
    "\n",
    "如果任务是文档级别的 (document level, such as spam detection, sentiment analysis):<br>\n",
    "* Document representation -> **bag-of-words** or **doc2vec**;<br>\n",
    "\n",
    "如果任务是单词或短语级别的 (such as text generation, machine translation):<br>\n",
    "* World representation -> **glove** or **word2vec**.\n",
    "\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Bag-of-words: treat each document as an un-ordered collection or bag-of-words.\n",
    "\n",
    "To obtain a bag-of-words from raw text: \n",
    "* cleaning;\n",
    "* normalizing;\n",
    "* splitting;\n",
    "* stemming;\n",
    "* lemmatization;\n",
    "* ...\n",
    "\n",
    "Keeping lots of un-ordered of seprate sets is inefficient. (Due to different sizes, different words, words duplication).\n",
    "\n",
    "Better representation: document -> vector of numbers (representing words occured times in a document):\n",
    "* collect all the unique words in corpus;\n",
    "* form a vocabulary;\n",
    "* form the vector element positions or columns of a table;\n",
    "* count number of occurrences of each word in each document;\n",
    "* enter value in the respective column.\n",
    "\n",
    "Now, this table can be a considered as a **Document-Term Matrix**, illustrating the relationship between documents in rows, and words or terms in columns.\n",
    "\n",
    "To evaluate the similarity between documents, using **cosine similarity** instead of dot product.\n",
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Drawbacks of bags-of-word: treat every word as being equally important.\n",
    "\n",
    "TF-IDF added the documents into consideration (**assigning weights** to words that signify their relevance in documents): <br>\n",
    "tfidt(t,d,D)=tf(t,d) * idf(t,D)<br>\n",
    "* tf: term freqenct\n",
    "* idf: inverse document frequency\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "So far, we look at the representations that tried to characterize an **entire document** or **collection of words** as on unit. However, the inferences we can make are also typically at a document level:\n",
    "* documents mixture of topics;\n",
    "* documents similarity;\n",
    "* documents sentiment;\n",
    "\n",
    "For deeper analysis, a numerical representation for each word is necessary.\n",
    "\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "Treat each word like a class, and assign it a vector that has 1 in a single pre-determined position for that word and 0 everywhere else.\n",
    "\n",
    "Similar with the bag-of-word idea, but only keep a single word in each bag and build a vector for it.\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "One-Hot encoding breaks down when we have a large vocabulary to deal with. So a word representation that control the size of our word representations by limiting it to a fixed-size vector is needed.\n",
    "\n",
    "We want to find an embedding for each word in some vector space and to exhibit some desired properties. Such a representation for a variaty of purposes like finding synonyms and analogies, identifying concepts around which words are clustered, classifying words as positive, negative, neutral...\n",
    "\n",
    "By combining word vector, we can get another way of representing documents as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* design a model;\n",
    "* fit its parameters;\n",
    "* training data using optimization procedure;\n",
    "* make predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Core idea: a model that is able to **1.** predict a given word, given neighboring words, or vice versa: **2.** predict neighboring words for a given word is likely to capture the contextual meanings of words very well.\n",
    "\n",
    "Word2Vec functions:\n",
    "1. given neighboring words, called continuous bag of words (CBOW);\n",
    "2. given the middle word, called skip-gram.\n",
    "\n",
    "#### Skip-gram model:\n",
    "1. Pick any word in a sentence; \n",
    "2. convert it into a one-hot encoded vector;\n",
    "3. feed it into a neural network (probabilistic model);\n",
    "4. predict a few surrounding words;\n",
    "5. train & optimize\n",
    "\n",
    "Continuous Bag of Words is using the same strategy.\n",
    "\n",
    "Properties:\n",
    "* Robust, distributed representation; (each word is distributed throughout the vector)\n",
    "* Vector size is independent of vacabulary; (unlike bag-of-words, where the size grows with the number of unique words)\n",
    "* Once trained, store in lookup table;\n",
    "* deep learning ready. (RNNs, also possible to use RNNs to learn better word embeddings)\n",
    "\n",
    "More: optimize using Hierarchical Softmax to present output word, using Spare Cross Entropy to compute loss...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "\n",
    "Word2Vec is one type of forward embedding. GloVe tries to directly optimize the vector representation of each word just using co-occurrence statistics, unlike Word2Vec which sets up an ancillary prediction task.\n",
    "\n",
    "1. probably that word $j$ appears in the context of word $i$ is computed; (simply that word $j$ is present in the vicinity of word $i$)\n",
    "2. count all such occurrences if $i$ and $j$ in our text collection;\n",
    "3. normalize account to get a probability;\n",
    "4. initialize a random vector for each word; (two sets of vectors: 1 for the word when it is acting as a context; 2 for the traget)\n",
    "5. for any pair of words $ij$, compute the dot product of their word vectors, to be equal to their co-occurrence probability;\n",
    "6. using co-occurence prob as goal and teratively optimize these word vectors;\n",
    "7. the result should be a set of vectors that capture the similarities and differences between individual words.\n",
    "\n",
    "In another point of view, factorizing the co-occurrence probability matrix into two smaller matrices;\n",
    "\n",
    "Why CO-occurrence Probabilities?<br>\n",
    "Given a large corpus, find that the ratio of $P(solid|ice)/P(solid/steam)$ greater than 1, and $P(water|ice)/P(water/steam)$ near to 1. So co-occurrence probabilities matrix already exhibit some of the properties we want to capture.<br>\n",
    "In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities.<br>\n",
    "co-occurrence probabilities matrix is huge and its values are typically very low (using logorithm values).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for Deep-learning\n",
    "\n",
    "Distributional Hypothesis: words occured in the same contexts tend to have similiar meanings. If a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together.\n",
    "\n",
    "How to capture similarities and differences in the same embedding? Add another dimensional. The more dimensions we can capture in your word vector, the more expressive that representaion will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "t-distributed Stochastic Neighbor Embedding. (Dimensional Reduction).\n",
    "\n",
    "It tries to maintain relative distances between objects. It's a great choice for visualizing word embeddings, and it effectivelty preserves the linear substractures and the relationships that have been learned by the embedding model.\n",
    "\n",
    "t-SNE also workds on other kinds of data, such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
