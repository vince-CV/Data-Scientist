{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts content -> Feature Extraction -> ?\n",
    "\n",
    "### Textual information\n",
    "\n",
    "取决于使用的模型类型：<br>\n",
    "* Graph based model to extract insights -> symbolic nodes with relationship (**WordNet**);<br>\n",
    "* Statistical model -> numerical representation.<br>\n",
    "\n",
    "如果任务是文档级别的 (document level, such as spam detection, sentiment analysis):<br>\n",
    "* Document representation -> **bag-of-words** or **doc2vec**;<br>\n",
    "\n",
    "如果任务是单词或短语级别的 (such as text generation, machine translation):<br>\n",
    "* World representation -> **glove** or **word2vec**.\n",
    "\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Bag-of-words: treat each document as an un-ordered collection or bag-of-words.\n",
    "\n",
    "To obtain a bag-of-words from raw text: \n",
    "* cleaning;\n",
    "* normalizing;\n",
    "* splitting;\n",
    "* stemming;\n",
    "* lemmatization;\n",
    "* ...\n",
    "\n",
    "Keeping lots of un-ordered of seprate sets is inefficient. (Due to different sizes, different words, words duplication).\n",
    "\n",
    "Better representation: document -> vector of numbers (representing words occured times in a document):\n",
    "* collect all the unique words in corpus;\n",
    "* form a vocabulary;\n",
    "* form the vector element positions or columns of a table;\n",
    "* count number of occurrences of each word in each document;\n",
    "* enter value in the respective column.\n",
    "\n",
    "Now, this table can be a considered as a **Document-Term Matrix**, illustrating the relationship between documents in rows, and words or terms in columns.\n",
    "\n",
    "To evaluate the similarity between documents, using **cosine similarity** instead of dot product.\n",
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Drawbacks of bags-of-word: treat every word as being equally important.\n",
    "\n",
    "TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。\n",
    "\n",
    "TF-IDF有两层意思，一层是\"词频\"（Term Frequency，缩写为TF），另一层是\"逆文档频率\"（Inverse Document Frequency，缩写为IDF）。\n",
    "\n",
    "TF-IDF added the documents into consideration (**assigning weights** to words that signify their relevance in documents): <br>\n",
    "tfidt(t,d,D)=tf(t,d) * idf(t,D)<br>\n",
    "* tf: term freqenct\n",
    "* idf: inverse document frequency\n",
    "\n",
    "假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。\n",
    "\n",
    "但这样又会遇到了另一个问题，我们可能发现\"量化\"、\"系统\"、\"架构\"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，**IDF会给常见的词较小的权重**，它的大小与一个词的常见程度成反比。\n",
    "\n",
    "**当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词**。\n",
    "\n",
    "算法步骤：\n",
    "1. 计算词频TF(某词在文章中出现的次数/文章总词数);\n",
    "2. 计算逆文档频率IDF：<br>\n",
    "    此时需要语料库(corpus), 用于模拟语言的使用环境：<br>\n",
    "    IDF = log(corpus中的文档总数/(包含该词的文档数+1))\n",
    "3. 计算TF-IDF:<br>\n",
    "    TF-IDF = TF * IDF\n",
    "\n",
    "TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。\n",
    "\n",
    "TF-IDF的优点是**简单快速，而且容易理解**。缺点是有时候**用词频来衡量文章中的一个词的重要性不够全面**，有时候重要的词出现的可能不够多，而且这种计算**无法体现位置信息，无法体现词在上下文的重要性**。(如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So far, we look at the representations that tried to characterize an **entire document** or **collection of words** as on unit. However, the inferences we can make are also typically at a document level:\n",
    "* documents mixture of topics;\n",
    "* documents similarity;\n",
    "* documents sentiment;\n",
    "\n",
    "For deeper analysis, a numerical representation for each word is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Treat each word like a class, and assign it a vector that has 1 in a single pre-determined position for that word and 0 everywhere else.\n",
    "\n",
    "Similar with the bag-of-word idea, but only keep a single word in each bag and build a vector for it.\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "One-Hot encoding breaks down when we have a large vocabulary to deal with. So a word representation that control the size of our word representations by limiting it to a fixed-size vector is needed.\n",
    "\n",
    "We want to find an embedding for each word in some vector space and to exhibit some desired properties. Such a representation for a variaty of purposes like finding synonyms and analogies, identifying concepts around which words are clustered, classifying words as positive, negative, neutral...\n",
    "\n",
    "By combining word vector, we can get another way of representing documents as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* design a model;\n",
    "* fit its parameters;\n",
    "* training data using optimization procedure;\n",
    "* make predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Core idea: a model that is able to **1.** predict a given word, given neighboring words, or vice versa: **2.** predict neighboring words for a given word is likely to capture the contextual meanings of words very well.\n",
    "\n",
    "Word2Vec functions:\n",
    "1. given neighboring words, called continuous bag of words (CBOW);\n",
    "2. given the middle word, called skip-gram.\n",
    "\n",
    "#### Skip-gram model:\n",
    "1. Pick any word in a sentence; \n",
    "2. convert it into a one-hot encoded vector;\n",
    "3. feed it into a neural network (probabilistic model);\n",
    "4. predict a few surrounding words;\n",
    "5. train & optimize\n",
    "\n",
    "Continuous Bag of Words is using the same strategy.\n",
    "\n",
    "Properties:\n",
    "* Robust, distributed representation; (each word is distributed throughout the vector)\n",
    "* Vector size is independent of vacabulary; (unlike bag-of-words, where the size grows with the number of unique words)\n",
    "* Once trained, store in lookup table;\n",
    "* deep learning ready. (RNNs, also possible to use RNNs to learn better word embeddings)\n",
    "\n",
    "More: optimize using Hierarchical Softmax to present output word, using Spare Cross Entropy to compute loss...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "\n",
    "Word2Vec is one type of forward embedding. GloVe tries to directly optimize the vector representation of each word just using co-occurrence statistics, unlike Word2Vec which sets up an ancillary prediction task.\n",
    "\n",
    "1. probably that word $j$ appears in the context of word $i$ is computed; (simply that word $j$ is present in the vicinity of word $i$)\n",
    "2. count all such occurrences if $i$ and $j$ in our text collection;\n",
    "3. normalize account to get a probability;\n",
    "4. initialize a random vector for each word; (two sets of vectors: 1 for the word when it is acting as a context; 2 for the traget)\n",
    "5. for any pair of words $ij$, compute the dot product of their word vectors, to be equal to their co-occurrence probability;\n",
    "6. using co-occurence prob as goal and teratively optimize these word vectors;\n",
    "7. the result should be a set of vectors that capture the similarities and differences between individual words.\n",
    "\n",
    "In another point of view, factorizing the co-occurrence probability matrix into two smaller matrices;\n",
    "\n",
    "Why CO-occurrence Probabilities?<br>\n",
    "Given a large corpus, find that the ratio of $P(solid|ice)/P(solid/steam)$ greater than 1, and $P(water|ice)/P(water/steam)$ near to 1. So co-occurrence probabilities matrix already exhibit some of the properties we want to capture.<br>\n",
    "In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities.<br>\n",
    "co-occurrence probabilities matrix is huge and its values are typically very low (using logorithm values).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for Deep-learning\n",
    "\n",
    "Distributional Hypothesis: words occured in the same contexts tend to have similiar meanings. If a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together.\n",
    "\n",
    "How to capture similarities and differences in the same embedding? Add another dimensional. The more dimensions we can capture in your word vector, the more expressive that representaion will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "t-distributed Stochastic Neighbor Embedding. (Dimensional Reduction).\n",
    "\n",
    "It tries to maintain relative distances between objects. It's a great choice for visualizing word embeddings, and it effectivelty preserves the linear substractures and the relationships that have been learned by the embedding model.\n",
    "\n",
    "t-SNE also workds on other kinds of data, such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
